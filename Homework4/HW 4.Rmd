---
title: "HW 4"
author: "Si Chen"
date: "10/31/2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

#4.8

##a)
```{r}
SpecificGravity <- c(0.499,0.558,0.604,0.441,0.550,0.528,0.418,0.480,0.406,0.467)
MoistureContent <- c(11.1,8.9,8.8,8.9,8.8,9.9,10.7,10.5,10.5,10.7)
Strength <- c(11.14,12.74,13.13,11.51,12.38,12.60,11.13,11.70,11.02,11.41)
plot(SpecificGravity, Strength, main = "Strength and Specific Gravity", xlab = "Specific Gravity", ylab = "Strength")
plot(MoistureContent, Strength, main = "Strength and Moisture Content", xlab = "Moisture Content", ylab = "Strength")
```

Moisture(8.9,11.51), i.e., observation No. 4 seems to be influential.


##b)
```{r}
X= as.matrix(data.frame(c(rep(1,10)),SpecificGravity,MoistureContent))
H <- X %*% solve(t(X) %*% X) %*% t(X)
H[4,4]
H[4,4] > 2 * (2 + 1) / 10
```

Observation No. 4 is influential.

##c)
```{r}
fit_strength <- lm(Strength ~ SpecificGravity + MoistureContent)
cooks.distance(fit_strength)[4]
cooks.distance(fit_strength)[4] > qf(0.2,3,7)
```

Observation No. 4 is influential.

##d)
```{r}
fit_str <- lm(Strength ~ SpecificGravity + MoistureContent)
summary(fit_str)
SpecificGravity_new <- SpecificGravity[c(-4)]
MoistureContent_new <- MoistureContent[c(-4)]
Strength_new <- Strength[c(-4)]
fit_strnew <- lm(Strength_new ~ SpecificGravity_new + MoistureContent_new)
summary(fit_strnew)
```

The R-squared improves and the fit changes.

#4.10
##a)
```{r}
x_1 <- c(rep(8,3),rep(0,3),rep(2,3),rep(0,3))
x_2 <- c(rep(1,3),rep(0,3),rep(7,3),rep(0,3))
x_3 <- c(rep(1,3),rep(9,3),rep(0,6))
x_4 <- c(1,rep(0,2),rep(1,6),rep(10,3))  
predictor1 <- data.frame(x_1,x_2,x_3,x_4)
cor(predictor1)
```

All absolute values of correlations in the correlation matrix are less than 0.5 and thus there is not an indication of multicollinearity.

##b)
```{r}
solve(cor(predictor1))
```

$VIF_1$ is 178.29, 
$VIF_2$ is 158.05, 
$VIF_3$ is 257.91, 
$VIF_4$ is 289.38 (maximum)

VIF's indicate serious multicollinearity

#4.11
##a)
```{r}
x1 <- c(rep(1300,6),rep(1200,6),rep(1100,4))
x2 <- c(7.5,9.0,11.0,13.5,17.0,23.0,5.3,7.5,11.0,13.5,17.0,23.0,5.3,7.5,11.0,17.0)
x3 <- c(0.0120,0.0120,0.0115,0.0130,0.0135,0.0120,0.0400,0.0380,0.0320,0.0260,0.0340,0.0410,0.0840,0.0980,0.0920,0.0860)
y <- c(49.0,50.2,50.5,48.5,47.5,44.5,28.0,31.5,34.5,35.0,38.0,38.5,15.0,17.0,20.5,29.5)
predictor <- data.frame(x1,x2,x3)
plot(predictor)
cor(predictor)
```

Corr(x1,x3) = -0.958, they are largely negative-correlated.

##b)
```{r}
x12 <- x1 * x2
x23 <- x2 * x3
x13 <- x1 * x3
x11 <- x1 * x1
x22 <- x2 * x2
x33 <- x3 * x3
predictor_all <- data.frame(x1,x2,x3,x12,x13,x23,x11,x22,x33)
VIF <- solve(cor(predictor_all))
VIF
```

$VIF_1$ is 2856748.965, 
$VIF_2$ is 10956.1361, 
$VIF_3$ is 2017162.536, 
$VIF_{12}$ is 9802.9028,  
$VIF_{13}$ is 1428091.893, 
$VIF_{23}$ is 240.35938, 
$VIF_{11}$ is 2501944.625, 
$VIF_{22}$ is 65.73359, 
$VIF_{33}$ is 12667.0995

All VIFs are larger than 10. There is a clear indication of multicollinearity among predictors.

##c)
```{r}
x1_c <- x1 - mean(x1)
x2_c <- x2 - mean(x2)
x3_c <- x3 - mean(x3)
x12_c <- x1_c * x2_c
x23_c <- x2_c * x3_c
x13_c <- x1_c * x3_c
x11_c <- x1_c * x1_c
x22_c <- x2_c * x2_c
x33_c <- x3_c * x3_c
predictor_all_c <- data.frame(x1_c,x2_c,x3_c,x12_c,x13_c,x23_c,x11_c,x22_c,x33_c)
VIF_c <- solve(cor(predictor_all_c))
VIF_c
```

The centering makes the multicollinearity problem less severe.

#5.4

install.packages("glmnet")


```{r}
library(glmnet)
ridgecv = cv.glmnet(as.matrix(predictor_all), y, lambda = seq(0,100,0.1),alpha = 0)
plot(ridgecv)
```

```{r}
small.lambda.index <- which(ridgecv$lambda == ridgecv$lambda.min)
small.lambda.betas <- coef(ridgecv$glmnet.fit)[,small.lambda.index]
print(small.lambda.betas)
```

```{r}
lambdaridge = ridgecv$lambda.min
print(lambdaridge)
```

```{r}
ridgefit = glmnet(as.matrix(predictor_all), y, alpha = 0, lambda = seq(0,100,0.01))
plot(ridgefit, xvar = "lambda", main = "Coeffs of Ridge Regressions", xlab = expression("log_lambda"), ylab = "Coeff")
abline(h = 0); abline(v = log(ridgecv$lambda.min))
grid()
```

#5.5
```{r}
lassocv = cv.glmnet(as.matrix(predictor_all), y, alpha = 1, lambda = seq(0,10,0.01))
plot(lassocv)
```

```{r}
lambdalasso = lassocv$lambda.min
print(lambdalasso)
```

```{r}
lassofit = glmnet(as.matrix(predictor_all), y, alpha = 1, lambda = seq(0,10,0.01))
plot(lassofit, xvar = "lambda", label = TRUE, main = "Coeffs of Lasso Regression", xlab = expression("log_lambda"), ylab = "Coeff")
abline(h = 0)
abline(v = log(lassocv$lambda.min))
grid()
```

```{r}
small.lambda.index <- which(lassocv$lambda == lassocv$lambda.min)
small.lambda.betas <- coef(lassocv$glmnet.fit)[,small.lambda.index]
print(small.lambda.betas)
```

$\beta_1,\beta_2,\beta_3,\beta_{12},\beta_{33}\ $ are set to zero.